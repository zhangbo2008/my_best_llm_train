{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# nvidia p100 kaggle上运行.\n\n!git clone https://github.com/zhangbo2008/firefly\n%cd firefly\n!pip install bitsandbytes\n!pip install peft\n!pip install loguru","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-18T02:47:23.976776Z","iopub.execute_input":"2023-08-18T02:47:23.977036Z","iopub.status.idle":"2023-08-18T02:48:06.797746Z","shell.execute_reply.started":"2023-08-18T02:47:23.977011Z","shell.execute_reply":"2023-08-18T02:48:06.796438Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'firefly'...\nremote: Enumerating objects: 81, done.\u001b[K\nremote: Counting objects: 100% (81/81), done.\u001b[K\nremote: Compressing objects: 100% (52/52), done.\u001b[K\nremote: Total 81 (delta 30), reused 77 (delta 26), pack-reused 0\u001b[K\nReceiving objects: 100% (81/81), 1.37 MiB | 17.26 MiB/s, done.\nResolving deltas: 100% (30/30), done.\n/kaggle/working/firefly\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.41.1\nCollecting peft\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.30.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\nCollecting loguru\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: loguru\nSuccessfully installed loguru-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-=========参数写到这里得了:\n\n\n\n\n'''\n多卡设置\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node={num_gpus} train_qlora.py --train_args_file train_args/qlora/baichuan-7b-sft-qlora.json\\\n\n\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2  6.py\n\n'''\n\n#=======设置单卡, 应该单卡也够用.\nimport os\nos.system('CUDA_VISIBLE_DEVICES=0')\n\n\nar='tmp.json'\naaa=r\"\"\"\n{\n    \"output_dir\": \"output/firefly-chatglm2-6b\",\n    \"model_name_or_path\": \"THUDM/chatglm2-6b\",\n    \"train_file\": \"./data/dummy_data.jsonl\",\n    \"num_train_epochs\": 1,\n    \"per_device_train_batch_size\": 2,\n    \"gradient_accumulation_steps\": 1,\n    \"learning_rate\": 2e-4,\n    \"max_seq_length\": 1024,\n    \"logging_steps\": 300,\n    \"save_steps\": 500,\n    \"save_total_limit\": 1,\n    \"lr_scheduler_type\": \"constant_with_warmup\",\n    \"warmup_steps\": 3000,\n    \"lora_rank\": 64,\n    \"lora_alpha\": 16,\n    \"lora_dropout\": 0.05,\n\n    \"gradient_checkpointing\": true,\n    \"disable_tqdm\": false,\n    \"optim\": \"paged_adamw_32bit\",\n    \"seed\": 42,\n    \"fp16\": true,\n    \"report_to\": \"tensorboard\",\n    \"dataloader_num_workers\": 0,\n    \"save_strategy\": \"steps\",\n    \"weight_decay\": 0,\n    \"max_grad_norm\": 0.3,\n    \"remove_unused_columns\": false\n}\n\n\n\n\n\n\n\n\"\"\"\nwith open('tmp.json','w') as f:\n    f.write(aaa)\nimport bitsandbytes\n\n\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, AdaLoraConfig,prepare_model_for_kbit_training\nfrom transformers import (\n    set_seed,\n    HfArgumentParser,\n    TrainingArguments,\n    AutoModelForCausalLM\n)\nimport argparse\nfrom loguru import logger\nimport os\nfrom os.path import join\nimport torch\nimport bitsandbytes as bnb\nfrom collections import defaultdict\n\nfrom component.collator import SFTDataCollator\nfrom component.dataset import SFTDataset, ChatGLM2SFTDataset\nfrom component.argument import QLoRAArguments\nfrom component.trainer import LoRATrainer\nfrom component.loss import TargetLMLoss\n\n\nif 1:\n\n\n\n    # 进行一些配置和检查\n\n\n\n    train_args_file = ar\n    # 读取训练的参数配置\n    parser = HfArgumentParser((QLoRAArguments, TrainingArguments))\n    # 解析得到自定义参数，以及自带参数\n\n    \n\n\n\n\n\n\n\n\n\n\n\n    args, training_args = parser.parse_json_file(json_file=train_args_file)\n    # 创建输出目录\n    if not os.path.exists(training_args.output_dir):\n        os.makedirs(training_args.output_dir)\n    # logger.add(join(training_args.output_dir, 'train.log'))\n    # logger.info(\"train_args:{}\".format(training_args))\n    # 设置随机种子\n    set_seed(training_args.seed)\n    # args, training_args = setup_everything()\n    # 加载各种组件\n\n\n    logger.info('Initializing components...')\n    # 下面的设置至关重要，否则无法多卡训练\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    print('是否ddp',ddp)\n    training_args.ddp_find_unused_parameters = False\n    device_map = \"auto\"\n    # if we are in a distributed setting, we need to set the device map and max memory per device\n    if os.environ.get('LOCAL_RANK') is not None:\n        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n        device_map = {'': local_rank}\n    device_map = {'': 0}\n    # 加载模型\n    model_old = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        device_map=device_map,\n        load_in_4bit=True,           #########???????????????这么加载训练精度很低吧.....\n        torch_dtype=torch.float16,\n        trust_remote_code=True,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n        ),\n    )\n    \n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T02:48:06.803910Z","iopub.execute_input":"2023-08-18T02:48:06.806290Z","iopub.status.idle":"2023-08-18T02:51:44.134386Z","shell.execute_reply.started":"2023-08-18T02:48:06.806246Z","shell.execute_reply":"2023-08-18T02:51:44.132942Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\u001b[32m2023-08-18 02:48:22.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mInitializing components...\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"是否ddp False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f41067a5c69476eae3396d3da6d5047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)iguration_chatglm.py:   0%|          | 0.00/2.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71d6d6fab6164baebd01a2b33f72e3f7"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b636261c2222497096b31e2e8e0e7144"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)main/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20bbf36ab4aa4ff88150b0b07b8a2fc8"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ebbefc6a3f426a8dc3fe3a9707157a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82469b4713d14167bf6dde20388846a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b398a5031704cf0a1a3f007e88bde3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b219c29c0eec44beaf0c44978b1da107"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67cc1b72ce374161a93f296ea90f5926"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ed0080a22414ac5b9f7667f82150f9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"814e4025bc234b778ccdf1d5cd099919"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b2cc8b4533d45c192977ec75e72dbab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cbb7a1b185548bc939d91c0a0c74a86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd8160a10bc043cc8d35c46b02658f93"}},"metadata":{}}]},{"cell_type":"code","source":"\nif 1:\n# 加载tokenzier\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=True,\n        # llama不支持fast\n        use_fast=False if model_old.config.model_type == 'llama' else True\n    )\n    # QWenTokenizer比较特殊，pad_token_id、bos_token_id、eos_token_id均为None。eod_id对应的token为<|endoftext|>\n    if tokenizer.__class__.__name__ == 'QWenTokenizer':\n        tokenizer.pad_token_id = tokenizer.eod_id\n        tokenizer.bos_token_id = tokenizer.eod_id\n        tokenizer.eos_token_id = tokenizer.eod_id\n    # ChatGLMTokenizer不需要设置，仅设置其他tokenizer\n    elif tokenizer.__class__.__name__ != 'ChatGLMTokenizer':\n        assert tokenizer.eos_token_id is not None\n        assert tokenizer.bos_token_id is not None\n        tokenizer.pad_token_id = tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n\n    # # 部分tokenizer没有pad_token_id\n    # if tokenizer.pad_token_id is None:\n    #     tokenizer.pad_token_id = tokenizer.unk_token_id\n    # # 部分tokenizer的pad_token_id与eos_token_id相同，如InternLM，会导致无法计算eos_token_id的loss。将pad_token_id设为unk_token_id\n    # if tokenizer.pad_token_id == tokenizer.eos_token_id and tokenizer.unk_token_id is not None:\n    #     tokenizer.pad_token_id = tokenizer.unk_token_id\n    # # 如果两者相同，模型训练时不会计算eos_token_id的loss\n    # if tokenizer.pad_token_id == tokenizer.eos_token_id:\n    #     raise Exception('pad_token_id should not be equal to eos_token_id')\n\n    # casts all the non int8 modules to full precision (fp32) for stability\n    model_old = prepare_model_for_kbit_training(model_old, use_gradient_checkpointing=training_args.gradient_checkpointing)\n\n    \n    print(f'memory footprint of model_old: {model_old.get_memory_footprint()/(1024*1024*1024)} GB')\n    # 找到所有需要插入adapter的全连接层\n    # target_modules = find_all_linear_names(model)\n\n    \n    \n    config = LoraConfig(\n    task_type='CAUSAL_LM', inference_mode=False,\n    r=8,\n    lora_alpha=32, lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]\n    )\n    \n    \n    \n    \n    model = get_peft_model(model_old, config)\n    model.print_trainable_parameters()\n    model.config.torch_dtype = torch.float16\n\n    import torch\n    import torch.nn as nn\n\n    class Loss(object):\n        \"\"\"\n        所有loss的类父类\n        \"\"\"\n        def __call__(self, model, inputs, training_args, return_outputs=False):\n            \"\"\"\n            todo label smoothing\n            用于计算loss。\n            看源码发现，return_outputs=True为train时调用，return_outputs=False为eval和predict调用\n            :param model: 模型\n            :param inputs: 模型输入，dict\n            :param training_args: 训练配置参数\n            :param return_outputs:是否返回模型的输出\n            :return:\n            \"\"\"\n            raise NotImplemented\n    class TargetLMLoss(Loss):\n\n        def __init__(self, ignore_index):\n            super().__init__()\n            self.ignore_index = ignore_index\n            self.loss_fn = nn.CrossEntropyLoss(ignore_index=ignore_index)\n\n        def __call__(self, model, inputs, training_args, return_outputs=False):\n            input_ids = inputs['input_ids']\n            attention_mask = inputs['attention_mask']\n            target_mask = inputs['target_mask']\n            # 模型前馈预测\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n            logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs[0]\n\n            # 将labels中不属于target的部分，设为ignore_index，只计算target部分的loss\n            labels = torch.where(target_mask == 1, input_ids, self.ignore_index)\n            shift_logits = logits[..., :-1, :].contiguous() # 因为我们生成的logits最后一个\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n            return (loss, outputs) if return_outputs else loss\n\n\n\n\n    # 初始化损失函数 ########################!!!!!!!!!!!!!!!!!!!!!\n    loss_func = TargetLMLoss(ignore_index=-100)\n\n    # 指加载训练集\n    if model.config.model_type == 'chatglm':\n        train_dataset = ChatGLM2SFTDataset(args.train_file, tokenizer, args.max_seq_length)\n    else:\n        train_dataset = SFTDataset(args.train_file, tokenizer, args.max_seq_length)\n    data_collator = SFTDataCollator(tokenizer, args.max_seq_length)\n\n    # 初始化Trainer\n    trainer = LoRATrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_loss=loss_func\n    )\nif 1:\n    logger.info(\"*** starting training ***\")\n    train_result = trainer.train()\n    # 保存最好的checkpoint\n    final_save_path = join(training_args.output_dir, 'final')\n    print('保存模型')\n    trainer.save_model(final_save_path)  # Saves the tokenizer too\n    # 保存训练指标\n    metrics = train_result.metrics\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T03:16:40.655188Z","iopub.execute_input":"2023-08-18T03:16:40.655783Z","iopub.status.idle":"2023-08-18T03:19:57.697712Z","shell.execute_reply.started":"2023-08-18T03:16:40.655745Z","shell.execute_reply":"2023-08-18T03:19:57.696646Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89298bd7e10b4e208eafe7ee088823d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85dc6bd492f645348fefb23ab2e616d4"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"630f8b6234b7409ead66b037fcad07c2"}},"metadata":{}},{"name":"stdout","text":"memory footprint of model_old: 4.644905149936676 GB\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-08-18 03:16:46.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcomponent.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mLoading data: ./data/dummy_data.jsonl\u001b[0m\n\u001b[32m2023-08-18 03:16:46.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcomponent.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mthere are 33 data in dataset\u001b[0m\n\u001b[32m2023-08-18 03:16:46.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1m*** starting training ***\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,949,696 || all params: 3,390,261,248 || trainable%: 0.05750872447219737\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [17/17 02:57, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"保存模型\n***** train metrics *****\n  epoch                    =        1.0\n  total_flos               =   589882GF\n  train_loss               =     6.1835\n  train_runtime            = 0:03:11.15\n  train_samples_per_second =      0.173\n  train_steps_per_second   =      0.089\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nif 1: # 合并大模型. 用于推理.    \n    model_name_or_path = args.model_name_or_path\n    adapter_name_or_path = final_save_path\n    save_path = 'checkpoint/firefly-baichuan-7b-qlora-sft-merge'\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name_or_path,\n        trust_remote_code=True\n    )\n#     model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         trust_remote_code=True,\n#         low_cpu_mem_usage=True,\n#         torch_dtype=torch.float16,\n#         device_map='auto'\n#     )\n    model_new = PeftModel.from_pretrained(model_old, adapter_name_or_path)\n#     model = model.merge_and_unload() ##########==========好像不用合并.  两个文件一起用也挺好哈!!!!!!!!!!\n#     print(model)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T03:27:03.325738Z","iopub.execute_input":"2023-08-18T03:27:03.326150Z","iopub.status.idle":"2023-08-18T03:27:22.365515Z","shell.execute_reply.started":"2023-08-18T03:27:03.326118Z","shell.execute_reply":"2023-08-18T03:27:22.364504Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"测试一下是否加载成功\n突发紧急情况很难预测,但是可以采取一些应对措施来减轻影响和缓解紧张局势。以下是一些应对突发紧急情况的建议:\n\n1. 保持冷静:在紧急情况下,保持冷静非常重要。不要惊慌,试图控制自己的情绪,清晰地思考下一步该怎么做。\n\n2. 优先考虑安全:如果感到自己或他人处于危险中,优先考虑安全。寻求帮助,如果可能的话,离开现场,或者寻求安全的避难所。\n\n3. 寻求专业帮助:如果感到自己或他人处于危险中,或者不知道如何应对,寻求专业帮助是非常重要的。可以拨打当地的急救电话或紧急求助热线,或者联系当地的应急服务机构。\n\n4. 准备好应对计划:在平时,建议制定一个应对计划,包括一些基本的应急物品、急救用品、通讯设备等。这有助于在紧急情况下更快地做出反应,更有效地应对。\n\n5. 学习基本急救技能:在紧急情况下,具备基本的急救技能可以帮助拯救他人。例如,如何进行心肺复苏、如何给伤口止血等。如果可能的话,最好接受相关的急救培训。\n\n6. 保持警觉:在紧急情况下,保持警觉可以帮助发现潜在的危险和机会,并及时采取行动。\n\n应对突发紧急情况需要冷静、明智和快速的反应。尽可能地采取行动,寻求帮助,并保持冷静,这将有助于您做出正确的决策,帮助自己和他人安全度过难关。\n","output_type":"stream"}]},{"cell_type":"code","source":"print('测试一下是否加载成功')\nresponse,history= model_new.chat(tokenizer,query='如何应对突发紧急情况？',history=[])\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2023-08-18T03:27:37.256576Z","iopub.execute_input":"2023-08-18T03:27:37.257118Z","iopub.status.idle":"2023-08-18T03:27:52.196472Z","shell.execute_reply.started":"2023-08-18T03:27:37.257078Z","shell.execute_reply":"2023-08-18T03:27:52.195352Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"测试一下是否加载成功\n突发紧急情况很难预测,但是下面是一些应对突发紧急情况的建议:\n\n1. 保持冷静:在紧急情况下,保持冷静非常重要。不要惊慌,试图控制自己的情绪,清晰地思考下一步该怎么做。\n\n2. 寻求帮助:如果遇到紧急情况,尽快寻求帮助。可以拨打紧急电话,如110、120等,或者与身边的亲友联系寻求帮助。\n\n3. 提供必要的信息:拨打紧急电话时,一定要提供必要的信息,例如所在的位置、事件的性质、人员伤亡情况等。这些信息可以帮助急救人员快速做出反应。\n\n4. 遵守指示:当接到紧急电话后,一定要遵守急救人员的指示。如果需要等待急救人员的到来,请耐心等待,并确保自己和他人的安全。\n\n5. 准备应对措施:在平时,可以准备一些应对紧急情况的基本物品,例如急救箱、食品、水、手机充电器等。这些物品可以帮助自己在紧急情况下更好地应对。\n\n6. 学习基本急救技能:在紧急情况下,掌握基本的急救技能可以挽救生命。例如,如何进行心肺复苏、如何给伤口止血等。\n\n应对突发紧急情况需要冷静、快速的反应和有效的应对措施。平时多加练习,提高自己的应对能力,可以帮助更好地应对紧急情况。\n","output_type":"stream"}]}]}